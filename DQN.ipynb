{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP Q NETWORK-Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwRRcc5J9iHc",
        "colab_type": "code",
        "outputId": "5966fc1a-8216-49d3-c81e-c2dd7d5aec13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from gym.envs.registration import register, spec\n",
        "from collections import deque\n",
        "from pandas import DataFrame, Series"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "443-B4R79vqG",
        "colab_type": "code",
        "outputId": "1876a739-8ff4-4e43-94a1-ca75b3b0ae6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "EPISODES = 2048\n",
        "EPSILON = 0.95\n",
        "EPSILON_DECAY = 0.95 \n",
        "EPSILON_MIN = 0.2 \n",
        "LEARNING_RATE = 0.01 \n",
        "GAMMA = 0.9 \n",
        "BATCH_SIZE = 32 #can be customized to 64, better fit with 32 obtained\n",
        "\n",
        "ACTION_LEFT = 0\n",
        "ACTION_DOWN = 1\n",
        "ACTION_RIGHT = 2\n",
        "ACTION_UP = 3\n",
        "ACTION_DEFAULT = None\n",
        "ACTION_TEXT = {\n",
        "    ACTION_LEFT: 'left',\n",
        "    ACTION_DOWN: 'down',\n",
        "    ACTION_RIGHT: 'right',\n",
        "    ACTION_UP: 'up'\n",
        "}\n",
        "\"This code has to be rerun in another session as gym library does not allow registering any custom environment twice\"\n",
        "from gym.envs.registration import register\n",
        "register(\n",
        "    id='Deterministic-4x4-FrozenLake-v0', # name given to this new environment\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', # env entry point\n",
        "    kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env\n",
        ")\n",
        "env = gym.make('Deterministic-4x4-FrozenLake-v0') # load the environment\n",
        "my_desk = [\n",
        "    \"GSFFF\",\n",
        "    \"FFFFF\",\n",
        "    \"FFFFG\",\n",
        "    \"FFFFF\",\n",
        "    \"FGFFG\"\n",
        "]\n",
        "class CustomizedFrozenLake(gym.envs.toy_text.frozen_lake.FrozenLakeEnv):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomizedFrozenLake, self).__init__(**kwargs)\n",
        "\n",
        "        for state in range(self.nS): # for all states\n",
        "            for action in range(self.nA): # for all actions\n",
        "                my_transitions = []\n",
        "                for (prob, next_state, _, is_terminal) in self.P[state][action]:\n",
        "                    row = next_state // self.ncol\n",
        "                    col = next_state - row * self.ncol\n",
        "                    tile_type = self.desc[row, col]\n",
        "                    if tile_type == b'F':\n",
        "                        reward = -1\n",
        "                    elif tile_type == b'G':\n",
        "                        reward = 10\n",
        "                    #else:\n",
        "                        #reward = 0\n",
        "\n",
        "                    my_transitions.append((prob, next_state, reward, is_terminal))\n",
        "                self.P[state][action] = my_transitions\n",
        "\n",
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    id='Stochastic-5x5-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'desc': my_desk, 'is_slippery': False})\n",
        "env = gym.make('Stochastic-5x5-FrozenLake-v0')\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "G\u001b[41mS\u001b[0mFFF\n",
            "FFFFF\n",
            "FFFFG\n",
            "FFFFF\n",
            "FGFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f2Kz7o-9vu7",
        "colab_type": "code",
        "outputId": "4c153913-c08c-4ff9-dd95-9a210a1bb6ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class DQNAgent():\n",
        "    def __init__(self):\n",
        "        self.env = self._build_env()\n",
        "        self.nb_status = self.env.observation_space.n\n",
        "        self.nb_action = self.env.action_space.n\n",
        "        self.memory = deque(maxlen=2048)\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_env(self): #Customized env setup\n",
        "        frozen_lake = 'Stochastic-5x5-FrozenLake-v0'\n",
        "        try:\n",
        "            spec(frozen_lake)\n",
        "        except:\n",
        "            register(id='Stochastic-5x5-FrozenLake-v0',\n",
        "                     entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "                     kwargs={'desc': my_desc, 'is_slippery': False})\n",
        "        return gym.make(frozen_lake)\n",
        "\n",
        "    def episode(self):\n",
        "        status = self.env.reset()\n",
        "\n",
        "        while True:\n",
        "            action = self._choose_action(status)\n",
        "            next_status, reward, done, info = self.env.step(action)\n",
        "            self.memory.append((status, action, reward, next_status, done))\n",
        "            status = next_status\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    def _choose_action(self, status, choose_best = False, return_probs = False):\n",
        "        global EPSILON\n",
        "\n",
        "        if_explore = False\n",
        "        if choose_best:\n",
        "            if_explore = False\n",
        "        else:\n",
        "            if_explore = np.random.uniform() < EPSILON\n",
        "\n",
        "        action = ACTION_DEFAULT\n",
        "        if if_explore:\n",
        "            # exploration\n",
        "            action = np.random.choice(self.nb_action)\n",
        "        else:\n",
        "            # exploitation\n",
        "            reward_pred = self.model.predict(self._one_hot_status(status))[0]\n",
        "            action = np.argmax(reward_pred)\n",
        "\n",
        "        if EPSILON > EPSILON_MIN:\n",
        "            EPSILON *= EPSILON_DECAY\n",
        "\n",
        "        return action if not return_probs else (action, reward_pred)\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        batches = random.sample(self.memory, BATCH_SIZE)\n",
        "        X = []\n",
        "        y = []\n",
        "        for status, action, reward, next_status, done in batches:\n",
        "            actual_reward = reward\n",
        "\n",
        "            if not done:\n",
        "                next_reward_pred = self.model.predict( self._one_hot_status(next_status))\n",
        "                actual_reward += GAMMA * np.max(next_reward_pred[0])\n",
        "\n",
        "            one_hot_status = self._one_hot_status(status)\n",
        "            reward_pred = self.model.predict(one_hot_status)\n",
        "            reward_pred[0][action] = actual_reward\n",
        "\n",
        "            X.append(one_hot_status[0])\n",
        "            y.append(reward_pred[0])\n",
        "\n",
        "        self.model.train_on_batch(DataFrame(X), DataFrame(y))\n",
        "        # self.model.fit(X, y, epochs=1, verbose=0)\n",
        "\n",
        "    def demo(self):\n",
        "        print(\"\\n------------- DEMO ----------------\")\n",
        "        decisions = []\n",
        "        rewards = []\n",
        "        for status in range(self.nb_status):\n",
        "            best_action, reward = self._choose_action(status, choose_best=True, return_probs=True)\n",
        "            decisions.append(best_action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "        for i in range(self.nb_status):\n",
        "            text = ''\n",
        "            if i==1:\n",
        "                text = 'START'\n",
        "            elif i in (0,14,21,24):\n",
        "                text = 'GOAL'\n",
        "            else:\n",
        "                text = ACTION_TEXT[decisions[i]]\n",
        "\n",
        "            print(\"{0:^7}\".format(text), end='')\n",
        "\n",
        "            if (i + 1) % 5 == 0:\n",
        "                print('\\n')\n",
        "\n",
        "        print('LEFT\\t\\tDOWN\\t\\tRIGHT\\t\\tUP')\n",
        "        for r in rewards:\n",
        "            print([i for i in r])\n",
        "\n",
        "    def _one_hot_status(self, status):\n",
        "        one_hot_status = np.zeros(self.nb_status)\n",
        "        one_hot_status[status] = 1\n",
        "        one_hot_status = np.expand_dims(one_hot_status, axis=0)\n",
        "        return one_hot_status\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(16, input_dim=self.nb_status, activation='relu'))\n",
        "        model.add(Dense(16, activation='relu'))\n",
        "        model.add(Dense(self.nb_action, activation='linear'))\n",
        "\n",
        "        model.compile(loss='mse', optimizer='adadelta')\n",
        "        model.summary()\n",
        "\n",
        "        return model\n",
        "\n",
        "def main():\n",
        "    agent = DQNAgent()\n",
        "\n",
        "    for i in range(EPISODES):\n",
        "        agent.episode()\n",
        "        agent.replay()\n",
        "\n",
        "        if (i+1) % 512 == 0:\n",
        "            agent.demo()\n",
        "            \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    print('\\nDone')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/toy_text/frozen_lake.py:112: RuntimeWarning: invalid value encountered in true_divide\n",
            "  isd /= isd.sum()\n",
            "/usr/local/lib/python3.6/dist-packages/gym/envs/toy_text/discrete.py:13: RuntimeWarning: invalid value encountered in greater\n",
            "  return (csprob_n > np_random.rand()).argmax()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16)                416       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 68        \n",
            "=================================================================\n",
            "Total params: 756\n",
            "Trainable params: 756\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "------------- DEMO ----------------\n",
            " GOAL   START  left   left   down  \n",
            "\n",
            " left   right  down   right  down  \n",
            "\n",
            " right   up    down   right  GOAL  \n",
            "\n",
            "  up    down   right  left   left  \n",
            "\n",
            " down   GOAL   left   right  GOAL  \n",
            "\n",
            "LEFT\t\tDOWN\t\tRIGHT\t\tUP\n",
            "[0.0, -3.7252903e-09, -9.313226e-10, -4.0745363e-10]\n",
            "[0.026883956, 0.027926492, -0.2081756, 0.05548093]\n",
            "[0.16299412, -0.042435415, -0.024069825, -0.029029256]\n",
            "[-0.026142433, -0.23784776, -0.088704094, -0.10066654]\n",
            "[-0.33659855, 0.025706047, -0.14406803, -0.14200369]\n",
            "[0.12137655, 0.060025744, 0.06183045, -0.09932299]\n",
            "[-0.18018717, -0.12429759, -0.010431042, -0.27416897]\n",
            "[-0.09432528, -0.04692413, -0.095963344, -0.07032873]\n",
            "[0.08967779, -0.042142354, 0.12336747, -0.1239941]\n",
            "[-0.07959615, 0.024529729, -0.04175861, -0.0247763]\n",
            "[0.062016807, 0.02022862, 0.13188526, -0.09485021]\n",
            "[-0.13127577, -0.00862902, -0.01202275, 0.023150962]\n",
            "[0.01305115, 0.0760047, -0.04525051, 0.0034894133]\n",
            "[0.029764276, -0.05809837, 0.10379493, 0.0028660449]\n",
            "[-0.023524888, -0.02000624, -0.11762434, -0.06505621]\n",
            "[0.0074093547, -0.04352007, 0.04105658, 0.042052124]\n",
            "[-0.3079422, -0.009171013, -0.1923568, -0.2293536]\n",
            "[0.0052046347, 0.05247982, 0.074958146, -0.11156538]\n",
            "[0.11399228, -0.17130932, -0.06638776, -0.0668566]\n",
            "[0.1130384, -0.02585829, -0.17499678, 0.037038647]\n",
            "[-0.016249675, 0.020873848, -0.029830104, -0.042138215]\n",
            "[-0.03306373, -0.024689697, -0.08251397, -0.03340717]\n",
            "[0.06681705, -0.07714953, -0.09243506, -0.10808613]\n",
            "[0.02140005, -0.027815484, 0.03517409, -0.116329335]\n",
            "[-0.3423802, 0.017804835, -0.14311832, -0.17612892]\n",
            "\n",
            "------------- DEMO ----------------\n",
            " GOAL   START  left   left   down  \n",
            "\n",
            " left   right  down   right  down  \n",
            "\n",
            " right   up    down   right  GOAL  \n",
            "\n",
            "  up    down   right  left   left  \n",
            "\n",
            " down   GOAL   left   right  GOAL  \n",
            "\n",
            "LEFT\t\tDOWN\t\tRIGHT\t\tUP\n",
            "[9.313226e-10, 0.0, 1.8626451e-09, -2.910383e-11]\n",
            "[0.026883963, 0.027926493, -0.20817558, 0.055480935]\n",
            "[0.16299413, -0.042435415, -0.024069823, -0.029029248]\n",
            "[-0.026142415, -0.23784776, -0.088704094, -0.10066652]\n",
            "[-0.33659852, 0.025706047, -0.14406802, -0.14200367]\n",
            "[0.121376574, 0.060025763, 0.06183046, -0.09932299]\n",
            "[-0.18018715, -0.12429756, -0.010431021, -0.27416894]\n",
            "[-0.094325274, -0.04692413, -0.095963344, -0.07032872]\n",
            "[0.089677796, -0.042142354, 0.12336747, -0.12399409]\n",
            "[-0.07959615, 0.024529746, -0.041758597, -0.024776299]\n",
            "[0.062016804, 0.020228634, 0.13188526, -0.094850205]\n",
            "[-0.13127576, -0.008629009, -0.0120227495, 0.02315097]\n",
            "[0.0130511485, 0.0760047, -0.045250505, 0.003489408]\n",
            "[0.029764265, -0.058098376, 0.10379491, 0.0028660516]\n",
            "[-0.023524888, -0.02000624, -0.11762435, -0.065056205]\n",
            "[0.007409349, -0.04352005, 0.04105658, 0.04205212]\n",
            "[-0.3079422, -0.009170983, -0.19235677, -0.2293536]\n",
            "[0.005204642, 0.05247983, 0.07495814, -0.11156538]\n",
            "[0.1139923, -0.17130932, -0.066387765, -0.0668566]\n",
            "[0.113038406, -0.025858276, -0.17499678, 0.037038643]\n",
            "[-0.016249662, 0.020873846, -0.029830106, -0.042138208]\n",
            "[-0.033063725, -0.024689697, -0.08251397, -0.033407174]\n",
            "[0.066817075, -0.07714952, -0.092435054, -0.10808611]\n",
            "[0.021400072, -0.027815491, 0.035174113, -0.11632933]\n",
            "[-0.3423802, 0.01780485, -0.14311829, -0.17612892]\n",
            "\n",
            "------------- DEMO ----------------\n",
            " GOAL   START  left   left   down  \n",
            "\n",
            " left   right  down   right  down  \n",
            "\n",
            " right   up    down   right  GOAL  \n",
            "\n",
            "  up    down   right  left   left  \n",
            "\n",
            " down   GOAL   left   right  GOAL  \n",
            "\n",
            "LEFT\t\tDOWN\t\tRIGHT\t\tUP\n",
            "[0.0, 0.0, 0.0, -2.910383e-11]\n",
            "[0.02688396, 0.02792648, -0.20817558, 0.05548094]\n",
            "[0.16299413, -0.042435423, -0.02406982, -0.029029245]\n",
            "[-0.026142422, -0.23784776, -0.088704094, -0.10066652]\n",
            "[-0.33659852, 0.02570604, -0.14406802, -0.14200367]\n",
            "[0.12137655, 0.06002574, 0.061830454, -0.09932298]\n",
            "[-0.18018714, -0.12429759, -0.010431031, -0.27416894]\n",
            "[-0.09432528, -0.046924137, -0.095963344, -0.07032872]\n",
            "[0.08967779, -0.042142354, 0.12336747, -0.12399409]\n",
            "[-0.07959613, 0.02452974, -0.041758608, -0.024776287]\n",
            "[0.062016826, 0.020228626, 0.13188526, -0.09485019]\n",
            "[-0.13127576, -0.008629013, -0.012022751, 0.023150973]\n",
            "[0.013051151, 0.0760047, -0.045250513, 0.0034894098]\n",
            "[0.029764242, -0.058098413, 0.1037949, 0.0028660644]\n",
            "[-0.023524888, -0.02000624, -0.11762434, -0.065056205]\n",
            "[0.0074093705, -0.043520063, 0.041056573, 0.042052127]\n",
            "[-0.3079422, -0.009171013, -0.19235681, -0.22935359]\n",
            "[0.005204647, 0.052479826, 0.07495814, -0.111565374]\n",
            "[0.11399228, -0.17130932, -0.066387765, -0.0668566]\n",
            "[0.1130384, -0.02585829, -0.17499678, 0.037038647]\n",
            "[-0.016249668, 0.02087384, -0.029830106, -0.042138208]\n",
            "[-0.033063725, -0.024689697, -0.08251397, -0.03340717]\n",
            "[0.06681707, -0.07714953, -0.09243506, -0.10808611]\n",
            "[0.02140008, -0.027815498, 0.035174113, -0.11632931]\n",
            "[-0.3423802, 0.017804824, -0.14311829, -0.17612892]\n",
            "\n",
            "------------- DEMO ----------------\n",
            " GOAL   START  left   left   down  \n",
            "\n",
            " left   right  down   right  down  \n",
            "\n",
            " right   up    down   right  GOAL  \n",
            "\n",
            "  up    down   right  left   left  \n",
            "\n",
            " down   GOAL   left   right  GOAL  \n",
            "\n",
            "LEFT\t\tDOWN\t\tRIGHT\t\tUP\n",
            "[0.0, 0.0, 0.0, -2.910383e-11]\n",
            "[0.02688396, 0.02792648, -0.20817558, 0.05548094]\n",
            "[0.16299413, -0.042435423, -0.02406982, -0.029029245]\n",
            "[-0.026142422, -0.23784776, -0.088704094, -0.10066652]\n",
            "[-0.33659852, 0.02570604, -0.14406802, -0.14200367]\n",
            "[0.12137655, 0.06002574, 0.061830454, -0.09932298]\n",
            "[-0.18018714, -0.12429759, -0.010431031, -0.27416894]\n",
            "[-0.09432528, -0.046924137, -0.095963344, -0.07032872]\n",
            "[0.08967779, -0.042142354, 0.12336747, -0.12399409]\n",
            "[-0.07959613, 0.02452974, -0.041758608, -0.024776287]\n",
            "[0.062016826, 0.020228626, 0.13188526, -0.09485019]\n",
            "[-0.13127576, -0.008629013, -0.012022751, 0.023150973]\n",
            "[0.013051151, 0.0760047, -0.045250513, 0.0034894098]\n",
            "[0.029764242, -0.058098413, 0.1037949, 0.0028660644]\n",
            "[-0.023524888, -0.02000624, -0.11762434, -0.065056205]\n",
            "[0.0074093705, -0.043520063, 0.041056573, 0.042052127]\n",
            "[-0.3079422, -0.009171013, -0.19235681, -0.22935359]\n",
            "[0.005204647, 0.052479826, 0.07495814, -0.111565374]\n",
            "[0.11399228, -0.17130932, -0.066387765, -0.0668566]\n",
            "[0.1130384, -0.02585829, -0.17499678, 0.037038647]\n",
            "[-0.016249668, 0.02087384, -0.029830106, -0.042138208]\n",
            "[-0.033063725, -0.024689697, -0.08251397, -0.03340717]\n",
            "[0.06681707, -0.07714953, -0.09243506, -0.10808611]\n",
            "[0.02140008, -0.027815498, 0.035174113, -0.11632931]\n",
            "[-0.3423802, 0.017804824, -0.14311829, -0.17612892]\n",
            "\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}